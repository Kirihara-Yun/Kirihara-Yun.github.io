<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大模型知识点速记 | Kirihara</title><meta name="author" content="Kirihara"><meta name="copyright" content="Kirihara"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大模型训练&#x2F;推理优化：深度碎片化知识点手册（50条） 以下知识点延续“核心概念+记忆口诀+关键细节”结构，新增训练进阶、推理深度优化、工具框架、前沿策略四大模块，每条可在2分钟内完成深度理解，适合碎片化系统积累。 一、训练优化：从基础到进阶（15条） 1. 混合精度训练（FP8进阶）  核心逻辑：在FP16&#x2F;FP32基础上引入8位浮点数（FP8），进一步压缩计算与内存开销，通过双格式适配平衡速度与">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型知识点速记">
<meta property="og:url" content="https://kirihara-yun.github.io/2023/08/10/transformer/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E9%80%9F%E8%AE%B0/index.html">
<meta property="og:site_name" content="Kirihara">
<meta property="og:description" content="大模型训练&#x2F;推理优化：深度碎片化知识点手册（50条） 以下知识点延续“核心概念+记忆口诀+关键细节”结构，新增训练进阶、推理深度优化、工具框架、前沿策略四大模块，每条可在2分钟内完成深度理解，适合碎片化系统积累。 一、训练优化：从基础到进阶（15条） 1. 混合精度训练（FP8进阶）  核心逻辑：在FP16&#x2F;FP32基础上引入8位浮点数（FP8），进一步压缩计算与内存开销，通过双格式适配平衡速度与">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kirihara-yun.github.io/img/huiyuanai.png">
<meta property="article:published_time" content="2023-08-10T02:00:00.000Z">
<meta property="article:modified_time" content="2025-10-08T10:49:26.293Z">
<meta property="article:author" content="Kirihara">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="速记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kirihara-yun.github.io/img/huiyuanai.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大模型知识点速记",
  "url": "https://kirihara-yun.github.io/2023/08/10/transformer/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E9%80%9F%E8%AE%B0/",
  "image": "https://kirihara-yun.github.io/img/huiyuanai.png",
  "datePublished": "2023-08-10T02:00:00.000Z",
  "dateModified": "2025-10-08T10:49:26.293Z",
  "author": [
    {
      "@type": "Person",
      "name": "Kirihara",
      "url": "https://kirihara-yun.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/huiyuanai.png"><link rel="canonical" href="https://kirihara-yun.github.io/2023/08/10/transformer/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E9%80%9F%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: Kirihara","link":"链接: ","source":"来源: Kirihara","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型知识点速记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg" style="background-image: url(/img/fushishan.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/huiyuanai.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/fushishan.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://img2.baidu.com/it/u=1933325124,3739680221&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=500" alt="Logo"><span class="site-name">Kirihara</span></a><a class="nav-page-title" href="/"><span class="site-name">大模型知识点速记</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大模型知识点速记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-10T02:00:00.000Z" title="发表于 2023-08-10 10:00:00">2023-08-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-08T10:49:26.293Z" title="更新于 2025-10-08 18:49:26">2025-10-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>大模型训练/推理优化：深度碎片化知识点手册（50条）</h1>
<p>以下知识点延续“核心概念+记忆口诀+关键细节”结构，新增训练进阶、推理深度优化、工具框架、前沿策略四大模块，每条可在2分钟内完成深度理解，适合碎片化系统积累。</p>
<h2 id="一-训练优化：从基础到进阶-15条">一、训练优化：从基础到进阶（15条）</h2>
<h3 id="1-混合精度训练-fp8进阶">1. 混合精度训练（FP8进阶）</h3>
<ul>
<li><strong>核心逻辑</strong>：在FP16/FP32基础上引入8位浮点数（FP8），进一步压缩计算与内存开销，通过双格式适配平衡速度与精度。</li>
<li><strong>记忆口诀</strong>：8位算得快，双格式适配，延迟缩放保精度。</li>
<li><strong>关键细节</strong>：
<ul>
<li>两种FP8格式分工：前向计算用E4M3（更多尾数位，精度更高），反向梯度用E5M2（更多指数位，动态范围更大）；</li>
<li>依赖Delayed Scaling策略：通过历史最大值缓存估算缩放因子，避免实时计算带来的访存开销。</li>
</ul>
</li>
</ul>
<h3 id="2-梯度累积-进阶技巧">2. 梯度累积（进阶技巧）</h3>
<ul>
<li><strong>核心逻辑</strong>：小批量迭代累积梯度模拟大批量效果，解决显存与最优batch size的矛盾。</li>
<li><strong>记忆口诀</strong>：小batch攒梯度，N次一更新， lr乘N才等效。</li>
<li><strong>关键细节</strong>：
<ul>
<li>与梯度裁剪协同：累积过程中需避免梯度中途溢出，建议在每次backward后暂存梯度范数；</li>
<li>适用局限：不适用于需要batch统计信息的层（如BatchNorm），需改用GroupNorm替代。</li>
</ul>
</li>
</ul>
<h3 id="3-zero优化-三阶段细分">3. ZeRO优化（三阶段细分）</h3>
<ul>
<li><strong>核心逻辑</strong>：通过分区模型状态（优化器、梯度、参数）消除数据并行中的冗余显存占用。</li>
<li><strong>记忆口诀</strong>：零冗余分区，-stage加一级，显存省一级。</li>
<li><strong>关键细节</strong>：
<ul>
<li>ZeRO-1：仅分区优化器状态（如Adam的动量项），显存节省40%+；</li>
<li>ZeRO-2：增加梯度分区，支持更大批量训练，显存节省60%+；</li>
<li>ZeRO-3：全参数分区，配合CPU/NVMe卸载可运行超大规模模型（如1T参数）。</li>
</ul>
</li>
</ul>
<h3 id="4-3d并行-混合并行终极方案">4. 3D并行（混合并行终极方案）</h3>
<ul>
<li><strong>核心逻辑</strong>：融合数据并行、张量并行、流水线并行，从数据、模型结构、计算流程三维度拆分任务。</li>
<li><strong>记忆口诀</strong>：数据分样本，张量分矩阵，流水分层级，三管齐下装大模。</li>
<li><strong>关键细节</strong>：
<ul>
<li>数据并行负责样本拆分，张量并行拆分单层计算，流水线并行拆分模型层级；</li>
<li>GPT-3采用“数据+张量+流水线”3D并行，支撑175B参数训练。</li>
</ul>
</li>
</ul>
<h3 id="5-梯度裁剪-两种模式对比">5. 梯度裁剪（两种模式对比）</h3>
<ul>
<li><strong>核心逻辑</strong>：通过限制梯度规模防止训练过程中梯度爆炸导致的参数异常。</li>
<li><strong>记忆口诀</strong>：范数裁剪保整体，元素裁剪控局部，阈值调参防崩溃。</li>
<li><strong>关键细节</strong>：
<ul>
<li>L2范数裁剪：计算梯度向量的整体范数，超过阈值则等比缩放（适合Transformer）；</li>
<li>元素裁剪：直接限制单个梯度值的上下界（适合CNN等局部特征敏感模型）。</li>
</ul>
</li>
</ul>
<h3 id="6-优化器：adamw-vs-lion">6. 优化器：AdamW vs Lion</h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>核心机制</th>
<th>记忆口诀</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>AdamW</td>
<td>自适应学习率+权重衰减</td>
<td>自适应稳收敛，细调效果佳</td>
<td>精调阶段、长文本任务</td>
</tr>
<tr>
<td>Lion</td>
<td>符号梯度+动量更新</td>
<td>符号更新快，显存占用低</td>
<td>预热阶段、低资源训练</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>关键细节</strong>：Lion单步训练速度比AdamW快18%，显存占用低25%，但需配合更低学习率（1e-5）避免震荡。</li>
</ul>
<h3 id="7-学习率调度-三段式策略">7. 学习率调度（三段式策略）</h3>
<ul>
<li><strong>核心逻辑</strong>：分阶段动态调整学习率，平衡收敛速度与泛化能力。</li>
<li><strong>记忆口诀</strong>：预热防震荡，衰减降损失，微调收最优。</li>
<li><strong>关键细节</strong>：
<ul>
<li>预热阶段（1000-5000步）：从1e-7线性升至目标lr，避免初始大lr破坏参数分布；</li>
<li>衰减阶段：用Cosine衰减替代线性衰减，后期缓慢下降保留探索能力；</li>
<li>微调阶段：降至初始lr的1/10，稳定收敛至最优解。</li>
</ul>
</li>
</ul>
<h3 id="8-梯度检查点-时间换空间">8. 梯度检查点（时间换空间）</h3>
<ul>
<li><strong>核心逻辑</strong>：牺牲部分计算时间，仅保存关键层激活值，反向传播时重新计算中间结果。</li>
<li><strong>记忆口诀</strong>： checkpoint留关键，反向重算省显存，时间换空间。</li>
<li><strong>关键细节</strong>：
<ul>
<li>显存节省比例：可减少40%-60%激活值占用，但训练速度下降20%-30%；</li>
<li>最佳实践：在Transformer的每2-3层设置一个检查点，平衡显存与速度。</li>
</ul>
</li>
</ul>
<h3 id="9-权重初始化-transformer专属">9. 权重初始化（Transformer专属）</h3>
<ul>
<li><strong>核心逻辑</strong>：针对Transformer的残差连接与注意力机制设计初始化策略，避免梯度消失。</li>
<li><strong>记忆口诀</strong>： Xavier适配线性层，注意力缩放要除d，残差增益需控制。</li>
<li><strong>关键细节</strong>：
<ul>
<li>线性层用Xavier初始化：按输入输出维度调整方差，避免激活值分布偏移；</li>
<li>注意力分数除以√d_model：防止点积结果过大导致softmax饱和。</li>
</ul>
</li>
</ul>
<h3 id="10-batchnorm替代方案-训练显存优化">10. BatchNorm替代方案（训练显存优化）</h3>
<ul>
<li><strong>核心逻辑</strong>：用GroupNorm/LayerNorm替代BatchNorm，减少批量依赖与显存占用。</li>
<li><strong>记忆口诀</strong>：BatchNorm靠批量，小batch用Group，LayerNorm更通用。</li>
<li><strong>关键细节</strong>：
<ul>
<li>GroupNorm：将通道分组计算均值方差，不受batch size影响，显存占用比BatchNorm低30%；</li>
<li>大模型首选LayerNorm：每层独立归一化，适配动态batch场景（如对话训练）。</li>
</ul>
</li>
</ul>
<h3 id="11-数据预处理优化-动态padding">11. 数据预处理优化（动态Padding）</h3>
<ul>
<li><strong>核心逻辑</strong>：同一batch内按样本长度动态调整padding长度，减少无效计算。</li>
<li><strong>记忆口诀</strong>：动态pad随长变，无效计算减一半，缓存分组提效率。</li>
<li><strong>关键细节</strong>：
<ul>
<li>实现方式：按长度区间分组（如0-64、65-128），同组样本用相同padding；</li>
<li>配合缓存机制：预生成各长度区间的mask矩阵，避免实时计算。</li>
</ul>
</li>
</ul>
<h3 id="12-分布式通信优化-nvlink-vs-pcie">12. 分布式通信优化（NVLink vs PCIe）</h3>
<ul>
<li><strong>核心逻辑</strong>：选择高效通信链路减少多卡数据传输延迟。</li>
<li><strong>记忆口诀</strong>：NVLink速度快，PCIe成本低，多卡集群靠前者。</li>
<li><strong>关键细节</strong>：
<ul>
<li>NVLink带宽：单链路300GB/s，支持8卡全连接，适合张量并行密集通信；</li>
<li>PCIe 4.0带宽：单链路32GB/s，适合数据并行的低频大流量传输。</li>
</ul>
</li>
</ul>
<h3 id="13-标签平滑-泛化能力提升">13. 标签平滑（泛化能力提升）</h3>
<ul>
<li><strong>核心逻辑</strong>：软化硬标签（如0/1）为概率分布，减少模型对错误标签的过拟合。</li>
<li><strong>记忆口诀</strong>：标签加平滑，置信降一点，泛化强一点。</li>
<li><strong>关键细节</strong>：
<ul>
<li>公式：y_smoothed = (1 - ε)×y_true + ε/K（K为类别数）；</li>
<li>ε取值：分类任务用0.1-0.2，生成任务用0.05（避免模糊语义）。</li>
</ul>
</li>
</ul>
<h3 id="14-对抗训练-鲁棒性增强">14. 对抗训练（鲁棒性增强）</h3>
<ul>
<li><strong>核心逻辑</strong>：在输入中添加微小扰动，迫使模型学习更稳健的特征表示。</li>
<li><strong>记忆口诀</strong>：加扰造对抗，模型练抗压，鲁棒性不差。</li>
<li><strong>关键细节</strong>：
<ul>
<li>常用方法：FGSM（快速梯度符号法）生成扰动，扰动强度控制在ε=1e-3；</li>
<li>适用场景：对齐训练中的事实性增强（减少幻觉）。</li>
</ul>
</li>
</ul>
<h3 id="15-模型并行-张量并行细分">15. 模型并行（张量并行细分）</h3>
<ul>
<li><strong>核心逻辑</strong>：将Transformer层内的张量拆分到多卡，解决单层计算量过大问题。</li>
<li><strong>记忆口诀</strong>：QKV拆多头，矩阵分块算，结果拼回不丢精。</li>
<li><strong>关键细节</strong>：
<ul>
<li>拆分维度：注意力头维度（num_heads）或隐藏层维度（hidden_size）；</li>
<li>通信开销：拆分后仅需交换中间结果，比流水线并行的气泡损失更低。</li>
</ul>
</li>
</ul>
<h2 id="二-推理优化：从压缩到加速-15条">二、推理优化：从压缩到加速（15条）</h2>
<h3 id="1-后训练量化-ptq">1. 后训练量化（PTQ）</h3>
<ul>
<li><strong>核心逻辑</strong>：训练完成后对模型权重/激活值量化，用少量校准数据统计分布。</li>
<li><strong>记忆口诀</strong>：训完再量化，校准算参数，快部署精度降不多。</li>
<li><strong>关键细节</strong>：
<ul>
<li>流程：权重量化（离线计算缩放因子）→ 激活值量化（校准数据统计范围）→ 推理验证；</li>
<li>适用场景：快速部署、对精度要求中等的场景（如客服机器人）。</li>
</ul>
</li>
</ul>
<h3 id="2-量化感知训练-qat">2. 量化感知训练（QAT）</h3>
<ul>
<li><strong>核心逻辑</strong>：训练过程中嵌入量化模拟操作，让模型适应精度损失。</li>
<li><strong>记忆口诀</strong>：训练带量化，模型早适应，精度掉得少但耗时久。</li>
<li><strong>关键细节</strong>：
<ul>
<li>模拟量化：前向传播添加量化/反量化节点，反向传播计算量化误差梯度；</li>
<li>优势：INT8量化后精度损失可控制在1%以内，远超PTQ的3%-5%。</li>
</ul>
</li>
</ul>
<h3 id="3-kv缓存优化-滑动窗口">3. KV缓存优化（滑动窗口）</h3>
<ul>
<li><strong>核心逻辑</strong>：长序列推理时仅缓存最近的K/V张量，替代全序列缓存。</li>
<li><strong>记忆口诀</strong>：长文滑窗口，旧KV丢一边，显存省一半。</li>
<li><strong>关键细节</strong>：
<ul>
<li>实现原理：缓存大小固定为窗口长度（如2048 tokens），新token加入时剔除最早token；</li>
<li>适用模型：Llama 2、Qwen等支持长上下文的模型，可将10万token推理显存降低70%。</li>
</ul>
</li>
</ul>
<h3 id="4-算子融合-推理效率核心">4. 算子融合（推理效率核心）</h3>
<ul>
<li><strong>核心逻辑</strong>：将多个独立算子（如Linear+LayerNorm+GELU）合并为单一算子，减少内存读写。</li>
<li><strong>记忆口诀</strong>：算子串成块，读写少一半，速度翻一番。</li>
<li><strong>关键细节</strong>：
<ul>
<li>常见融合模式：Transformer层内“Attention+Add+Norm”融合、“Linear+激活函数”融合；</li>
<li>工具依赖：TensorRT自动融合静态算子，TVM支持自定义融合规则。</li>
</ul>
</li>
</ul>
<h3 id="5-推测解码-speculative-decoding">5. 推测解码（Speculative Decoding）</h3>
<ul>
<li><strong>核心逻辑</strong>：用小模型快速预测候选序列，大模型仅验证修正，减少大模型调用次数。</li>
<li><strong>记忆口诀</strong>：小模型猜，大模型改，快生成还保质量。</li>
<li><strong>关键细节</strong>：
<ul>
<li>流程：小模型生成K个候选token → 大模型批量验证 → 接受正确token并生成下一个；</li>
<li>加速比：可提升2-3倍推理速度，候选长度K通常设为5-10。</li>
</ul>
</li>
</ul>
<h3 id="6-剪枝-结构化vs非结构化">6. 剪枝（结构化vs非结构化）</h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>核心逻辑</th>
<th>记忆口诀</th>
<th>部署要求</th>
</tr>
</thead>
<tbody>
<tr>
<td>结构化剪枝</td>
<td>剪整个通道/层，保留模型结构</td>
<td>剪层剪通道，部署无压力</td>
<td>通用硬件支持</td>
</tr>
<tr>
<td>非结构化剪枝</td>
<td>剪单个权重，稀疏化连接</td>
<td>剪单个权重，压缩比更高</td>
<td>需稀疏计算硬件（如GPU稀疏核）</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>关键细节</strong>：剪枝后需微调恢复精度，结构化剪枝精度损失通常&lt;2%，非结构化可低至1%但部署成本高。</li>
</ul>
<h3 id="7-知识蒸馏-软标签进阶">7. 知识蒸馏（软标签进阶）</h3>
<ul>
<li><strong>核心逻辑</strong>：让学生模型学习老师模型的输出分布（软标签）与中间特征，而非仅硬标签。</li>
<li><strong>记忆口诀</strong>：学软分布，仿中间层，小模型有大能力。</li>
<li><strong>关键细节</strong>：
<ul>
<li>温度系数（T）：控制软标签平滑度，T越大分布越平，通常取2-5；</li>
<li>损失构成：蒸馏损失（占70%）+ 硬标签损失（占30%），平衡泛化与拟合。</li>
</ul>
</li>
</ul>
<h3 id="8-动态批处理-推理吞吐量优化">8. 动态批处理（推理吞吐量优化）</h3>
<ul>
<li><strong>核心逻辑</strong>：动态合并不同时刻的请求，替代静态批处理，减少等待开销。</li>
<li><strong>记忆口诀</strong>：请求动态合，不等长序列，吞吐提三倍。</li>
<li><strong>关键细节</strong>：
<ul>
<li>实现方式：按请求到达时间分批，每批满额或超时（如50ms）即处理；</li>
<li>工具支持：vLLM、TGI（Text Generation Inference）原生支持动态批处理。</li>
</ul>
</li>
</ul>
<h3 id="9-fp8推理-端到端优化">9. FP8推理（端到端优化）</h3>
<ul>
<li><strong>核心逻辑</strong>：训练用FP8的模型可直接以FP8推理，省去量化转换步骤。</li>
<li><strong>记忆口诀</strong>：训推同用8位，转换省时间，算力翻两倍。</li>
<li><strong>关键细节</strong>：
<ul>
<li>硬件依赖：H100 GPU的FP8 Tensor Core算力是FP16的2倍；</li>
<li>精度保障：通过高精度累加（FP32）抵消低精度计算误差。</li>
</ul>
</li>
</ul>
<h3 id="10-低秩分解-linformer实践">10. 低秩分解（Linformer实践）</h3>
<ul>
<li><strong>核心逻辑</strong>：用低秩矩阵投影压缩注意力中的Q/K/V张量，降低计算复杂度。</li>
<li><strong>记忆口诀</strong>：高秩矩阵拆低秩，注意力复杂度降线性，长文处理不卡顿。</li>
<li><strong>关键细节</strong>：
<ul>
<li>数学原理：将QK^T的O(n²d)复杂度降至O(nkd)（k≪n）；</li>
<li>Linformer实现：通过投影矩阵E/F将序列长度从n压缩到k（通常k=256）。</li>
</ul>
</li>
</ul>
<h3 id="11-flash-attention-内存高效注意力">11. Flash Attention（内存高效注意力）</h3>
<ul>
<li><strong>核心逻辑</strong>：通过分块计算与内存复用，避免注意力计算中的中间结果溢出显存。</li>
<li><strong>记忆口诀</strong>：分块算注意力，内存循环用，速度快还省空间。</li>
<li><strong>关键细节</strong>：
<ul>
<li>突破点：将注意力计算拆分为小块，利用GPU共享内存（Shared Memory）替代全局内存；</li>
<li>性能提升：相比标准注意力快2-4倍，显存占用降低90%。</li>
</ul>
</li>
</ul>
<h3 id="12-模型编译-tvm-mlir">12. 模型编译（TVM/MLIR）</h3>
<ul>
<li><strong>核心逻辑</strong>：将模型转换为中间表示（IR），通过算子优化、内存规划生成硬件专属代码。</li>
<li><strong>记忆口诀</strong>：模型转IR，编译优算子，硬件跑更快。</li>
<li><strong>关键细节</strong>：
<ul>
<li>优化步骤：算子融合→循环展开→内存对齐→指令调度；</li>
<li>适用场景：边缘设备部署（如ARM架构CPU），可提升30%-50%推理速度。</li>
</ul>
</li>
</ul>
<h3 id="13-稀疏推理-剪枝后加速">13. 稀疏推理（剪枝后加速）</h3>
<ul>
<li><strong>核心逻辑</strong>：利用硬件稀疏计算能力，跳过剪枝后的零值权重计算。</li>
<li><strong>记忆口诀</strong>：零值权重跳着算，稀疏硬件来加速，计算效率翻一番。</li>
<li><strong>关键细节</strong>：
<ul>
<li>稀疏度要求：NVIDIA A100支持4:1稀疏（每4个元素1个非零），H100支持2:1稀疏；</li>
<li>性能瓶颈：稀疏度低于50%时加速效果不明显。</li>
</ul>
</li>
</ul>
<h3 id="14-多模态模型推理优化-权重共享">14. 多模态模型推理优化（权重共享）</h3>
<ul>
<li><strong>核心逻辑</strong>：跨模态（图文/音视频）共享基础编码器权重，减少重复计算。</li>
<li><strong>记忆口诀</strong>：模态共享基础层，参数省一半，推理更高效。</li>
<li><strong>关键细节</strong>：
<ul>
<li>典型案例：CLIP的图文编码器共享Transformer主干，仅头部差异化；</li>
<li>优化点：共享层用INT8量化，差异化头部用FP16保留精度。</li>
</ul>
</li>
</ul>
<h3 id="15-推理缓存策略-多轮对话">15. 推理缓存策略（多轮对话）</h3>
<ul>
<li><strong>核心逻辑</strong>：缓存多轮对话中的历史KV张量，仅计算新轮次输入。</li>
<li><strong>记忆口诀</strong>：历史KV存起来，新话只算新增token，响应快一倍。</li>
<li><strong>关键细节</strong>：
<ul>
<li>缓存管理：超过最大长度时按FIFO策略剔除最早轮次；</li>
<li>精度注意：缓存需与推理精度一致（如INT8推理缓存INT8 KV）。</li>
</ul>
</li>
</ul>
<h2 id="三-工具与框架：落地实战必备-7条">三、工具与框架：落地实战必备（7条）</h2>
<h3 id="1-训练框架：deepspeed">1. 训练框架：DeepSpeed</h3>
<ul>
<li><strong>核心功能</strong>：基于PyTorch的分布式训练优化框架，主打显存高效与速度提升。</li>
<li><strong>记忆口诀</strong>：ZeRO省显存，混合精度快，卸载能装大模型。</li>
<li><strong>关键细节</strong>：
<ul>
<li>核心特性：ZeRO优化、FP8训练、梯度检查点自动集成；</li>
<li>适用场景：10B+参数模型训练，单卡24GB可训7B模型。</li>
</ul>
</li>
</ul>
<h3 id="2-训练框架：megatron-lm">2. 训练框架：Megatron-LM</h3>
<ul>
<li><strong>核心功能</strong>：NVIDIA推出的大模型训练框架，优化多卡并行效率。</li>
<li><strong>记忆口诀</strong>：3D并行强，张量拆分细，GPU利用率高。</li>
<li><strong>关键细节</strong>：
<ul>
<li>特色功能：支持张量并行内的算子融合，多卡通信优化；</li>
<li>代表案例：Llama、GPT-3等模型训练基础框架。</li>
</ul>
</li>
</ul>
<h3 id="3-推理框架：tensorrt">3. 推理框架：TensorRT</h3>
<ul>
<li><strong>核心功能</strong>：NVIDIA专属推理优化引擎，主打量化、算子融合与编译优化。</li>
<li><strong>记忆口诀</strong>：量化融算子，编译生最优码，GPU跑最快。</li>
<li><strong>关键细节</strong>：
<ul>
<li>优化流程：模型导入→量化校准→算子融合→引擎生成；</li>
<li>精度选项：支持FP32/FP16/INT8/FP8，INT8校准仅需50-100个样本。</li>
</ul>
</li>
</ul>
<h3 id="4-推理框架：vllm">4. 推理框架：vLLM</h3>
<ul>
<li><strong>核心功能</strong>：基于PagedAttention的高吞吐量推理框架，主打动态批处理。</li>
<li><strong>记忆口诀</strong>：分页存KV，动态合批快，吞吐超TGI。</li>
<li><strong>关键细节</strong>：
<ul>
<li>核心创新：PagedAttention将KV缓存按页管理，支持非连续内存分配；</li>
<li>性能表现：同等配置下吞吐量是TGI的2-4倍。</li>
</ul>
</li>
</ul>
<h3 id="5-量化工具：gptq">5. 量化工具：GPTQ</h3>
<ul>
<li><strong>核心功能</strong>：针对Transformer的后训练量化工具，支持INT4/INT8量化。</li>
<li><strong>记忆口诀</strong>：量化GPT类模型，INT4精度高，速度提升明显。</li>
<li><strong>关键细节</strong>：
<ul>
<li>技术原理：通过梯度下降优化量化误差，保留注意力层精度；</li>
<li>压缩比：7B模型INT4量化后仅占3GB显存，精度损失&lt;3%。</li>
</ul>
</li>
</ul>
<h3 id="6-剪枝工具：torchprune">6. 剪枝工具：TorchPrune</h3>
<ul>
<li><strong>核心功能</strong>：PyTorch生态剪枝工具，支持结构化与非结构化剪枝。</li>
<li><strong>记忆口诀</strong>：剪枝可视化，支持多策略，剪后易微调。</li>
<li><strong>关键细节</strong>：
<ul>
<li>剪枝策略：支持L1/L2正则剪枝、运动剪枝（Movement Pruning）；</li>
<li>联动功能：剪枝后自动生成微调脚本，恢复精度效率高。</li>
</ul>
</li>
</ul>
<h3 id="7-peft工具：lora">7. PEFT工具：LoRA</h3>
<ul>
<li><strong>核心功能</strong>：参数高效微调工具，通过低秩矩阵适配大模型。</li>
<li><strong>记忆口诀</strong>：冻结主干网，训练小矩阵，微调省显存。</li>
<li><strong>关键细节</strong>：
<ul>
<li>数学原理：将权重更新ΔW分解为U（m×r）和V（r×n），r通常取8-64；</li>
<li>显存优势：7B模型微调仅需5GB显存，比全量微调省90%。</li>
</ul>
</li>
</ul>
<h2 id="四-高频易混概念辨析-8条">四、高频易混概念辨析（8条）</h2>
<h3 id="1-fp8混合精度-vs-fp16混合精度">1. FP8混合精度 vs FP16混合精度</h3>
<ul>
<li><strong>核心差异</strong>：精度粒度与硬件依赖不同。
<ul>
<li>FP16：动态范围足够（6.1e-5至6.5e4），无需复杂缩放策略，适配所有现代GPU；</li>
<li>FP8：动态范围有限（E4M3：2e-8至2e4），需Delayed Scaling，仅支持H100等新硬件。</li>
</ul>
</li>
</ul>
<h3 id="2-qat-vs-ptq">2. QAT vs PTQ</h3>
<table>
<thead>
<tr>
<th>维度</th>
<th>QAT（量化感知训练）</th>
<th>PTQ（后训练量化）</th>
</tr>
</thead>
<tbody>
<tr>
<td>时机</td>
<td>训练中嵌入量化模拟</td>
<td>训练完成后处理</td>
</tr>
<tr>
<td>精度</td>
<td>损失小（&lt;2%）</td>
<td>损失较大（3%-8%）</td>
</tr>
<tr>
<td>成本</td>
<td>需重新训练，耗时久</td>
<td>无需训练，几分钟完成</td>
</tr>
<tr>
<td>适用场景</td>
<td>高精度要求场景（如医疗）</td>
<td>快速部署场景（如客服）</td>
</tr>
</tbody>
</table>
<h3 id="3-模型并行-vs-张量并行">3. 模型并行 vs 张量并行</h3>
<ul>
<li><strong>模型并行</strong>：按层拆分模型（如Transformer的encoder层1-10在卡1，11-20在卡2），解决单卡装不下模型问题；</li>
<li><strong>张量并行</strong>：拆分单层内的张量（如QKV矩阵分块），解决单层计算量过大问题，通信更密集。</li>
</ul>
<h3 id="4-zero优化-vs-模型并行">4. ZeRO优化 vs 模型并行</h3>
<ul>
<li><strong>ZeRO优化</strong>：基于数据并行的分区策略，不改变模型结构，仅消除冗余显存；</li>
<li><strong>模型并行</strong>：改变模型部署结构，拆分层或张量，适用于单卡完全装不下模型的场景。</li>
</ul>
<h3 id="5-梯度裁剪-vs-梯度累积">5. 梯度裁剪 vs 梯度累积</h3>
<ul>
<li><strong>梯度裁剪</strong>：主动限制梯度规模，解决“梯度爆炸”，是稳定性保障手段；</li>
<li><strong>梯度累积</strong>：被动攒梯度模拟大batch，解决“显存不足”，是资源适配手段。</li>
</ul>
<h3 id="6-剪枝-vs-低秩分解">6. 剪枝 vs 低秩分解</h3>
<ul>
<li><strong>剪枝</strong>：删除无用参数（减少参数数量），可能破坏模型结构连续性；</li>
<li><strong>低秩分解</strong>：用小矩阵逼近大矩阵（减少参数维度），保留模型结构完整性。</li>
</ul>
<h3 id="7-kv缓存-vs-梯度检查点">7. KV缓存 vs 梯度检查点</h3>
<ul>
<li><strong>KV缓存</strong>：推理阶段使用，缓存中间计算结果避免重复计算，牺牲显存换速度；</li>
<li><strong>梯度检查点</strong>：训练阶段使用，删除中间激活值减少显存，牺牲速度换空间。</li>
</ul>
<h3 id="8-动态批处理-vs-静态批处理">8. 动态批处理 vs 静态批处理</h3>
<ul>
<li><strong>静态批处理</strong>：固定batch大小，请求需等待凑齐批次，长序列会拖慢整体；</li>
<li><strong>动态批处理</strong>：按时间/请求数动态调整批次，无需等待，吞吐量提升2-3倍。</li>
</ul>
<h2 id="五-进阶策略与前沿方向-5条">五、进阶策略与前沿方向（5条）</h2>
<h3 id="1-自蒸馏-self-distillation">1. 自蒸馏（Self-Distillation）</h3>
<ul>
<li><strong>核心逻辑</strong>：让模型自身的教师版本（如多轮微调后的模型）教学生版本（量化/剪枝后的模型）。</li>
<li><strong>记忆口诀</strong>：自己教自己，无需额外老师，压缩精度掉得少。</li>
<li><strong>关键细节</strong>：适用于无优质教师模型的场景，学生模型精度可接近原模型的95%。</li>
</ul>
<h3 id="2-持续学习-catastrophic-forgetting缓解">2. 持续学习（Catastrophic Forgetting缓解）</h3>
<ul>
<li><strong>核心逻辑</strong>：通过知识重放、参数正则化等方式，在微调新任务时保留原有能力。</li>
<li><strong>记忆口诀</strong>：新任务学知识，旧任务常回放，避免全忘记。</li>
<li><strong>关键细节</strong>：常用方法：弹性权重巩固（EWC）给旧任务关键参数加正则约束。</li>
</ul>
<h3 id="3-rlhf三阶段优化">3. RLHF三阶段优化</h3>
<ul>
<li><strong>核心逻辑</strong>：通过监督微调（SFT）、奖励模型（RM）训练、PPO优化，让模型对齐人类偏好。</li>
<li><strong>记忆口诀</strong>：先教后评再优化，三阶段对齐，回答更合规。</li>
<li><strong>关键细节</strong>：PPO阶段需平衡奖励值与原模型偏差，避免模型“过度迎合”导致事实性下降。</li>
</ul>
<h3 id="4-多模态联合优化">4. 多模态联合优化</h3>
<ul>
<li><strong>核心逻辑</strong>：跨模态（图文/音视频）共享优化策略，如统一量化、联合蒸馏。</li>
<li><strong>记忆口诀</strong>：模态共享优化策，参数计算双节省，跨域推理更高效。</li>
<li><strong>关键细节</strong>：CLIP通过联合蒸馏将图文模型体积压缩4倍，推理速度提升3倍。</li>
</ul>
<h3 id="5-边缘端优化-int2量化">5. 边缘端优化（INT2量化）</h3>
<ul>
<li><strong>核心逻辑</strong>：极致低精度量化（INT2），适配手机、IoT等边缘设备。</li>
<li><strong>记忆口诀</strong>：2位极致压缩，精度靠校准，边缘能运行。</li>
<li><strong>关键细节</strong>：需配合蒸馏使用，7B模型INT2量化后可压缩至1GB以内，适用于离线语音助手。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kirihara-yun.github.io">Kirihara</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kirihara-yun.github.io/2023/08/10/transformer/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E9%80%9F%E8%AE%B0/">https://kirihara-yun.github.io/2023/08/10/transformer/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E9%80%9F%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kirihara-yun.github.io" target="_blank">Kirihara</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a><a class="post-meta__tags" href="/tags/%E9%80%9F%E8%AE%B0/">速记</a></div><div class="post-share"><div class="social-share" data-image="/img/huiyuanai.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/08/22/transformer/QKV/" title="一文搞懂QKV"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">一文搞懂QKV</div></div><div class="info-2"><div class="info-item-1">QKV：Transformer注意力机制的核心范式 1. QKV的起源与本质 QKV（Query-Key-Value，查询-键-值）并非独立存在的结构，而是Transformer模型为实现结构化注意力交互而设计的核心组件。其本质是对Transformer输入向量（词嵌入与位置编码的融合表示）进行三次独立线性变换，从而将输入的单一语义表征拆分为三种功能差异化的向量，为后续注意力权重的计算与信息聚合提供范式支撑。 2. QKV的前置输入：Transformer的基础表征 Transformer处理的是固定长度的离散序列（如文本中的词汇、图像中的patch），需先将序列元素转化为连续的高维向量，该过程由词嵌入与位置编码两步完成，最终形成QKV的生成基础——输入向量XXX。 2.1 词嵌入（Word Embedding） 词嵌入的核心作用是将离散的符号化Token（如文本中的单词“apple”）映射至连续的低维实数向量空间，生成具有语义区分度的表征。其数学定义为：设序列长度为LLL，模型维度（即嵌入向量维度）为dmodeld_{\text{model}}dmodel​，则词嵌入矩阵E∈R...</div></div></div></a><a class="pagination-related" href="/2023/08/10/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/416%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/" title="416分割等和子集"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">416分割等和子集</div></div><div class="info-2"><div class="info-item-1">416分割等和子集 题目描述 给你一个只包含正整数的非空数值nums,请你判断是否可以将这个数组分为两个子集，使其元素和相等。 问题分析 (1)若一个数组nums可以被分为两个子集，那么数组和必定是偶数。 (2)问题可以转化为，是否存在一个子集，它的元素和是数组和的一半，这是典型的子集和target问题。 解题核心与思路 这是典型的背包问题： （1）每个元素可以选择一次 （2）背包容量为数组和的一半 （3）是否存在一个子集，它的元素和等于背包容量 转化为动态规划问题： （1）dp[i]表示是否可以用数组中的元素组成和为i的子集。 （2）初始化dp[0] = true，表示和为0的子集总是存在的。 （3）迭代数组nums中的元素n，再从target反向遍历到n,更新dp[j] = dp[j]||dp[j-n]。 解题代码 1234567891011121314151617181920class Solution &#123;public:    bool canPartition(vector&lt;int&gt;&amp; nums) &#123;        int sum ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2023/10/20/transformer/%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8/" title="Transformer编码器-解码器双向协作模式：原理、设计与优化"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-20</div><div class="info-item-2">Transformer编码器-解码器双向协作模式：原理、设计与优化</div></div><div class="info-2"><div class="info-item-1">编码器-解码器双向协作模式：原理、设计、问题与解决方案 1. 引言：Transformer与编码器-解码器框架概述 自2017年Google团队在《Attention Is All You Need》中提出Transformer模型以来，其基于“注意力机制”的核心设计彻底改变了自然语言处理（NLP）、计算机视觉（CV）等领域的序列建模范式。Transformer的核心架构由编码器（Encoder） 和解码器（Decoder） 两部分组成，二者通过注意力机制实现信息交互，是机器翻译、文本摘要、对话生成等“序列到序列（Seq2Seq）”任务的核心载体。 在传统Seq2Seq模型（如RNN-based）中，编码器仅负责将输入序列编码为固定长度的“上下文向量”，再传递给解码器生成输出序列——这种“单向信息流动”模式在长序列或复杂任务中存在明显局限。随着研究推进，编码器-解码器双向协作模式逐渐成为主流：它打破了“编码器一次性输出、解码器被动接收”的限制，实现了编码器与解码器在生成过程中的动态信息交互（编码器根据解码器的当前状态调整输出，解码器根据编码器的动态反馈优化生成），显著提升了模型对...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/huiyuanai.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Kirihara</div><div class="author-info-description">喜欢您来</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Kirihara-Yun"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Kirihara-Yun" target="_blank" title=""><i class="fab fa-github"></i></a><a class="social-icon" href="/qinzhy26@mail2.sysu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">希望您天天开心</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">大模型训练&#x2F;推理优化：深度碎片化知识点手册（50条）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%EF%BC%9A%E4%BB%8E%E5%9F%BA%E7%A1%80%E5%88%B0%E8%BF%9B%E9%98%B6-15%E6%9D%A1"><span class="toc-text">一、训练优化：从基础到进阶（15条）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83-fp8%E8%BF%9B%E9%98%B6"><span class="toc-text">1. 混合精度训练（FP8进阶）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF-%E8%BF%9B%E9%98%B6%E6%8A%80%E5%B7%A7"><span class="toc-text">2. 梯度累积（进阶技巧）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-zero%E4%BC%98%E5%8C%96-%E4%B8%89%E9%98%B6%E6%AE%B5%E7%BB%86%E5%88%86"><span class="toc-text">3. ZeRO优化（三阶段细分）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3d%E5%B9%B6%E8%A1%8C-%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C%E7%BB%88%E6%9E%81%E6%96%B9%E6%A1%88"><span class="toc-text">4. 3D并行（混合并行终极方案）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA-%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="toc-text">5. 梯度裁剪（两种模式对比）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%9Aadamw-vs-lion"><span class="toc-text">6. 优化器：AdamW vs Lion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6-%E4%B8%89%E6%AE%B5%E5%BC%8F%E7%AD%96%E7%95%A5"><span class="toc-text">7. 学习率调度（三段式策略）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5%E7%82%B9-%E6%97%B6%E9%97%B4%E6%8D%A2%E7%A9%BA%E9%97%B4"><span class="toc-text">8. 梯度检查点（时间换空间）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96-transformer%E4%B8%93%E5%B1%9E"><span class="toc-text">9. 权重初始化（Transformer专属）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-batchnorm%E6%9B%BF%E4%BB%A3%E6%96%B9%E6%A1%88-%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E4%BC%98%E5%8C%96"><span class="toc-text">10. BatchNorm替代方案（训练显存优化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%BC%98%E5%8C%96-%E5%8A%A8%E6%80%81padding"><span class="toc-text">11. 数据预处理优化（动态Padding）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E5%88%86%E5%B8%83%E5%BC%8F%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96-nvlink-vs-pcie"><span class="toc-text">12. 分布式通信优化（NVLink vs PCIe）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91-%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E6%8F%90%E5%8D%87"><span class="toc-text">13. 标签平滑（泛化能力提升）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83-%E9%B2%81%E6%A3%92%E6%80%A7%E5%A2%9E%E5%BC%BA"><span class="toc-text">14. 对抗训练（鲁棒性增强）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E7%BB%86%E5%88%86"><span class="toc-text">15. 模型并行（张量并行细分）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%EF%BC%9A%E4%BB%8E%E5%8E%8B%E7%BC%A9%E5%88%B0%E5%8A%A0%E9%80%9F-15%E6%9D%A1"><span class="toc-text">二、推理优化：从压缩到加速（15条）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%90%8E%E8%AE%AD%E7%BB%83%E9%87%8F%E5%8C%96-ptq"><span class="toc-text">1. 后训练量化（PTQ）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%E8%AE%AD%E7%BB%83-qat"><span class="toc-text">2. 量化感知训练（QAT）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-kv%E7%BC%93%E5%AD%98%E4%BC%98%E5%8C%96-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-text">3. KV缓存优化（滑动窗口）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88-%E6%8E%A8%E7%90%86%E6%95%88%E7%8E%87%E6%A0%B8%E5%BF%83"><span class="toc-text">4. 算子融合（推理效率核心）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%8E%A8%E6%B5%8B%E8%A7%A3%E7%A0%81-speculative-decoding"><span class="toc-text">5. 推测解码（Speculative Decoding）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%89%AA%E6%9E%9D-%E7%BB%93%E6%9E%84%E5%8C%96vs%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96"><span class="toc-text">6. 剪枝（结构化vs非结构化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-%E8%BD%AF%E6%A0%87%E7%AD%BE%E8%BF%9B%E9%98%B6"><span class="toc-text">7. 知识蒸馏（软标签进阶）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86-%E6%8E%A8%E7%90%86%E5%90%9E%E5%90%90%E9%87%8F%E4%BC%98%E5%8C%96"><span class="toc-text">8. 动态批处理（推理吞吐量优化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-fp8%E6%8E%A8%E7%90%86-%E7%AB%AF%E5%88%B0%E7%AB%AF%E4%BC%98%E5%8C%96"><span class="toc-text">9. FP8推理（端到端优化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3-linformer%E5%AE%9E%E8%B7%B5"><span class="toc-text">10. 低秩分解（Linformer实践）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-flash-attention-%E5%86%85%E5%AD%98%E9%AB%98%E6%95%88%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">11. Flash Attention（内存高效注意力）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91-tvm-mlir"><span class="toc-text">12. 模型编译（TVM&#x2F;MLIR）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-%E7%A8%80%E7%96%8F%E6%8E%A8%E7%90%86-%E5%89%AA%E6%9E%9D%E5%90%8E%E5%8A%A0%E9%80%9F"><span class="toc-text">13. 稀疏推理（剪枝后加速）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96-%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="toc-text">14. 多模态模型推理优化（权重共享）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-%E6%8E%A8%E7%90%86%E7%BC%93%E5%AD%98%E7%AD%96%E7%95%A5-%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D"><span class="toc-text">15. 推理缓存策略（多轮对话）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-%E5%B7%A5%E5%85%B7%E4%B8%8E%E6%A1%86%E6%9E%B6%EF%BC%9A%E8%90%BD%E5%9C%B0%E5%AE%9E%E6%88%98%E5%BF%85%E5%A4%87-7%E6%9D%A1"><span class="toc-text">三、工具与框架：落地实战必备（7条）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6%EF%BC%9Adeepspeed"><span class="toc-text">1. 训练框架：DeepSpeed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6%EF%BC%9Amegatron-lm"><span class="toc-text">2. 训练框架：Megatron-LM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%EF%BC%9Atensorrt"><span class="toc-text">3. 推理框架：TensorRT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%EF%BC%9Avllm"><span class="toc-text">4. 推理框架：vLLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E9%87%8F%E5%8C%96%E5%B7%A5%E5%85%B7%EF%BC%9Agptq"><span class="toc-text">5. 量化工具：GPTQ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%89%AA%E6%9E%9D%E5%B7%A5%E5%85%B7%EF%BC%9Atorchprune"><span class="toc-text">6. 剪枝工具：TorchPrune</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-peft%E5%B7%A5%E5%85%B7%EF%BC%9Alora"><span class="toc-text">7. PEFT工具：LoRA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-%E9%AB%98%E9%A2%91%E6%98%93%E6%B7%B7%E6%A6%82%E5%BF%B5%E8%BE%A8%E6%9E%90-8%E6%9D%A1"><span class="toc-text">四、高频易混概念辨析（8条）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-fp8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6-vs-fp16%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6"><span class="toc-text">1. FP8混合精度 vs FP16混合精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-qat-vs-ptq"><span class="toc-text">2. QAT vs PTQ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C-vs-%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C"><span class="toc-text">3. 模型并行 vs 张量并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-zero%E4%BC%98%E5%8C%96-vs-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="toc-text">4. ZeRO优化 vs 模型并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA-vs-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-text">5. 梯度裁剪 vs 梯度累积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%89%AA%E6%9E%9D-vs-%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3"><span class="toc-text">6. 剪枝 vs 低秩分解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-kv%E7%BC%93%E5%AD%98-vs-%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-text">7. KV缓存 vs 梯度检查点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86-vs-%E9%9D%99%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-text">8. 动态批处理 vs 静态批处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-%E8%BF%9B%E9%98%B6%E7%AD%96%E7%95%A5%E4%B8%8E%E5%89%8D%E6%B2%BF%E6%96%B9%E5%90%91-5%E6%9D%A1"><span class="toc-text">五、进阶策略与前沿方向（5条）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%87%AA%E8%92%B8%E9%A6%8F-self-distillation"><span class="toc-text">1. 自蒸馏（Self-Distillation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0-catastrophic-forgetting%E7%BC%93%E8%A7%A3"><span class="toc-text">2. 持续学习（Catastrophic Forgetting缓解）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-rlhf%E4%B8%89%E9%98%B6%E6%AE%B5%E4%BC%98%E5%8C%96"><span class="toc-text">3. RLHF三阶段优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%A4%9A%E6%A8%A1%E6%80%81%E8%81%94%E5%90%88%E4%BC%98%E5%8C%96"><span class="toc-text">4. 多模态联合优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%BE%B9%E7%BC%98%E7%AB%AF%E4%BC%98%E5%8C%96-int2%E9%87%8F%E5%8C%96"><span class="toc-text">5. 边缘端优化（INT2量化）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/" title="层归一化位置选择">层归一化位置选择</a><time datetime="2025-10-06T02:00:00.000Z" title="发表于 2025-10-06 10:00:00">2025-10-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/25/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/372%E8%B6%85%E7%BA%A7%E6%AC%A1%E6%96%B9/" title="372超级次方">372超级次方</a><time datetime="2025-09-24T16:00:00.000Z" title="发表于 2025-09-25 00:00:00">2025-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/25/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/436%E5%AF%BB%E6%89%BE%E5%8F%B3%E5%8C%BA%E9%97%B4/" title="436寻找右区间">436寻找右区间</a><time datetime="2025-09-24T16:00:00.000Z" title="发表于 2025-09-25 00:00:00">2025-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/20/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/memag/" title="MEMAGmemory-Efficient Graph Transformation via Access Pattern-Aware Optimization for DNNs">MEMAGmemory-Efficient Graph Transformation via Access Pattern-Aware Optimization for DNNs</a><time datetime="2025-09-20T08:00:00.000Z" title="发表于 2025-09-20 16:00:00">2025-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/18/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/318%E6%9C%80%E5%A4%A7%E5%8D%95%E8%AF%8D%E9%95%BF%E5%BA%A6%E4%B9%98%E7%A7%AF/" title="318. 最大单词长度乘积">318. 最大单词长度乘积</a><time datetime="2025-09-18T02:00:00.000Z" title="发表于 2025-09-18 10:00:00">2025-09-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/fushishan.jpg);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Kirihara</span></div><div class="footer_custom_text">你终于找到我啦(*^▽^*)</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>