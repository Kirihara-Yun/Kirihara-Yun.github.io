<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>层归一化位置选择 | Kirihara</title><meta name="author" content="Kirihara"><meta name="copyright" content="Kirihara"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本文详细介绍了层归一化位置选择的核心作用，包括调控训练稳定性、优化收敛效率和校准特征分布。">
<meta property="og:type" content="article">
<meta property="og:title" content="层归一化位置选择">
<meta property="og:url" content="https://kirihara-yun.github.io/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/index.html">
<meta property="og:site_name" content="Kirihara">
<meta property="og:description" content="本文详细介绍了层归一化位置选择的核心作用，包括调控训练稳定性、优化收敛效率和校准特征分布。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kirihara-yun.github.io/img/huiyuanai.png">
<meta property="article:published_time" content="2025-10-06T02:00:00.000Z">
<meta property="article:modified_time" content="2025-10-09T01:05:48.530Z">
<meta property="article:author" content="Kirihara">
<meta property="article:tag" content="层归一化">
<meta property="article:tag" content="位置选择">
<meta property="article:tag" content="Transformer架构">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kirihara-yun.github.io/img/huiyuanai.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "层归一化位置选择",
  "url": "https://kirihara-yun.github.io/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/",
  "image": "https://kirihara-yun.github.io/img/huiyuanai.png",
  "datePublished": "2025-10-06T02:00:00.000Z",
  "dateModified": "2025-10-09T01:05:48.530Z",
  "author": [
    {
      "@type": "Person",
      "name": "Kirihara",
      "url": "https://kirihara-yun.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/huiyuanai.png"><link rel="canonical" href="https://kirihara-yun.github.io/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: Kirihara","link":"链接: ","source":"来源: Kirihara","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '层归一化位置选择',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg" style="background-image: url(/img/fushishan.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/huiyuanai.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/fushishan.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://img2.baidu.com/it/u=1933325124,3739680221&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=500" alt="Logo"><span class="site-name">Kirihara</span></a><a class="nav-page-title" href="/"><span class="site-name">层归一化位置选择</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">层归一化位置选择</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-06T02:00:00.000Z" title="发表于 2025-10-06 10:00:00">2025-10-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-09T01:05:48.530Z" title="更新于 2025-10-09 09:05:48">2025-10-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>层归一化位置选择：Transformer架构中的关键设计决策</h1>
<h2 id="一-层归一化位置选择的核心定义">一、层归一化位置选择的核心定义</h2>
<p>在Transformer架构中，层归一化（LayerNorm）位置选择特指<strong>自注意力模块、前馈神经网络（FFN）等基础组件与残差连接构成的组合单元内</strong>，确定LayerNorm操作执行时机的设计决策。其核心本质是明确LayerNorm、残差连接与子模块计算的执行顺序，最终目标是保障模型训练稳定性、提升模型性能。</p>
<p>目前，该设计决策已形成两种主流核心范式，二者的核心差异在于LayerNorm作用的阶段：</p>
<ol>
<li><strong>Pre-Norm范式</strong>：遵循“归一化→子模块计算→残差连接”的顺序，LayerNorm直接作用于子模块的输入端，是当前大语言模型（LLM）的主流选择。</li>
<li><strong>Post-Norm范式</strong>：遵循“子模块计算→残差连接→归一化”的顺序，LayerNorm作用于残差连接的输出端，是传统Transformer（如原始BERT、GPT-1）的经典设计。</li>
</ol>
<p>以自注意力子模块为例，两种范式的计算逻辑可具体表示为：</p>
<ul>
<li><strong>Pre-Norm</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Output = x + Attention(Norm(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span><span class="mord mathdefault">p</span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br>
（先对输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>归一化，再输入注意力模块计算，最后与原始<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>残差融合）</li>
<li><strong>Post-Norm</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Output = Norm(x + Attention(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span><span class="mord mathdefault">p</span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br>
（先对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>执行注意力计算，再与原始<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>残差融合，最后对融合结果归一化）</li>
</ul>
<h2 id="二-层归一化位置选择的核心作用">二、层归一化位置选择的核心作用</h2>
<p>LayerNorm的位置并非单纯的“顺序差异”，而是直接影响模型训练动态与最终性能的关键变量，其核心作用可拆解为三大维度：</p>
<h3 id="2-1-调控模型训练稳定性">2.1 调控模型训练稳定性</h3>
<p>训练稳定性的核心差异源于<strong>残差路径的梯度是否受归一化干扰</strong>：</p>
<ul>
<li>Pre-Norm通过将归一化前置，使残差连接传递的梯度完全避开归一化因子的影响——原始输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>的梯度可直接沿残差路径传递至低层，无需经过归一化的“缩放操作”，从而确保深层模型的梯度能稳定传递至低层参数，避免梯度消失或爆炸。</li>
<li>Post-Norm则对残差融合后的输出强制缩放：归一化会压缩残差路径的特征方差，导致深层梯度经多层累积后呈“指数级衰减”，尤其在模型层数较深时，低层参数难以获得有效更新信号。</li>
</ul>
<h3 id="2-2-优化模型收敛效率">2.2 优化模型收敛效率</h3>
<p>收敛效率的差异本质是<strong>对内部协变量偏移（Internal Covariate Shift）的抑制时机不同</strong>：<br>
内部协变量偏移指模型训练中，各层输入分布随参数更新发生剧烈变化，导致子模块需频繁调整参数以适配新分布，显著拖慢收敛速度。</p>
<ul>
<li>Pre-Norm在子模块计算前即对输入标准化，从源头抑制了输入分布的偏移——子模块始终接收“分布稳定”的输入，无需频繁适配波动，使模型在训练前期即可快速进入梯度稳定区间。实验表明，Pre-Norm的收敛速度相比纯Post-Norm架构可提升40%。</li>
<li>Post-Norm仅在子模块输出后进行归一化，子模块内部仍需处理“分布偏移的原始输入”，参数更新效率较低，收敛周期更长。</li>
</ul>
<h3 id="2-3-校准中间层特征分布">2.3 校准中间层特征分布</h3>
<p>两种范式对中间层特征的“约束逻辑”不同，直接影响特征的可复用性与表达能力：</p>
<ul>
<li>Post-Norm通过对残差输出归一化，强制各层输出符合“标准高斯分布”（均值接近0、方差接近1），确保了中间层特征的“分布一致性”。这一特性使BERT等模型可任意抽取中间层特征（如[CLS]向量）作用于下游任务，无需额外适配分布。</li>
<li>Pre-Norm仅对“子模块输入”归一化，残差融合后的输出保留了原始特征的尺度差异——特征以“渐进式累积”的方式传递，虽能保留更细粒度的特征信息，但中间层特征分布不统一，若需复用需额外增加归一化层适配。</li>
</ul>
<h2 id="三-为何必须重视层归一化位置选择？">三、为何必须重视层归一化位置选择？</h2>
<p>LayerNorm位置的选择并非“可选优化”，而是由Transformer架构的核心矛盾与模型规模需求决定的，其根本原因可归结为两点：</p>
<h3 id="3-1-残差连接与layernorm的核心逻辑冲突">3.1 残差连接与LayerNorm的核心逻辑冲突</h3>
<p>残差连接与LayerNorm的设计目标存在天然矛盾，而位置选择直接决定了“谁妥协谁”：</p>
<ul>
<li>残差连接的核心目标是<strong>通过恒等映射传递原始特征</strong>：让浅层特征能直接传递至深层，避免深层特征“遗忘”浅层信息，同时保障梯度传递效率。</li>
<li>LayerNorm的核心目标是<strong>强制特征分布标准化</strong>：通过缩放与平移，压缩特征方差，稳定输入分布。</li>
</ul>
<p>在Post-Norm架构中，这种矛盾被放大：残差连接本应让特征方差“自然累积”以保留信息，但LayerNorm会强制压缩方差，导致残差连接的“梯度传递功能”失效——浅层信号的贡献度会以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>2</mn><msup><mo stretchy="false">)</mo><mrow><mi>L</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">(1/2)^{L/2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord">2</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span><span class="mord mtight">/</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>的速度衰减（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span>为模型层数），即层数越多，浅层信息与梯度的传递效率越低。<br>
而Pre-Norm通过“前置归一化”规避了这一冲突：归一化仅作用于子模块输入，不影响残差路径的原始特征传递，残差连接的核心功能得以保留。</p>
<h3 id="3-2-模型规模与训练可行性的适配需求">3.2 模型规模与训练可行性的适配需求</h3>
<p>随着模型层数从“十几层”向“上百层”演进，Post-Norm的局限性愈发明显，而Pre-Norm成为大规模模型训练的“必要条件”：</p>
<ul>
<li>原始Post-Norm架构的训练稳定性极差：当层数超过18层时，模型已出现明显的训练失败倾向（如梯度消失、loss震荡不收敛），需依赖“复杂度渐进（warmup）策略”“精细参数初始化”等额外手段才能勉强运行，且效果不稳定。</li>
<li>Pre-Norm具备“天然的深层适配性”：无需额外调优即可支持96层以上的模型训练（如GPT-3的部分结构、PaLM），其梯度传递的稳定性使“超深层模型”从“不可行”变为“可行”，直接推动了大模型的规模突破。</li>
</ul>
<h2 id="四-层归一化位置选择的作用机理">四、层归一化位置选择的作用机理</h2>
<p>LayerNorm位置之所以能产生显著影响，根源在于其改变了Transformer的<strong>前向特征流动路径</strong>与<strong>反向梯度传播路径</strong>，进而调控模型的训练动态。其核心作用机理可拆解为三点：</p>
<h3 id="4-1-梯度路径分离机制：pre-norm的核心优势">4.1 梯度路径分离机制：Pre-Norm的核心优势</h3>
<p>Pre-Norm的训练稳定性源于其“双路径梯度传递”设计，实现了“核心梯度”与“子模块梯度”的分离：</p>
<ul>
<li><strong>直接梯度路径（残差路径）</strong>：残差连接传递的梯度完全不经过归一化操作——原始输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>的梯度直接从输出端传递至输入端，无任何缩放因子干扰，确保低层参数能获得“强更新信号”。</li>
<li><strong>子模块梯度路径</strong>：经过归一化与子模块（如注意力）的梯度，仅作用于“子模块内部参数的调整”（如注意力权重、FFN权重），完全不影响残差路径的核心梯度流。</li>
</ul>
<p>这种“分离特性”确保了Pre-Norm的梯度范数不会因归一化操作而缩减，从根本上解决了深层模型的梯度消失问题。</p>
<h3 id="4-2-分布偏移的抑制时机：决定收敛效率的关键">4.2 分布偏移的抑制时机：决定收敛效率的关键</h3>
<p>LayerNorm的核心价值是“缓解内部协变量偏移”，但“抑制时机”直接决定了缓解效果：</p>
<ul>
<li>Pre-Norm的抑制逻辑是“防患于未然”：在子模块计算前即对输入标准化，使子模块始终接收“分布稳定”的输入——参数无需频繁调整以适配波动，更新效率高，收敛快。</li>
<li>Post-Norm的抑制逻辑是“事后补救”：子模块已处理完“分布偏移的输入”，参数已因偏移产生无效更新，此时再归一化仅能稳定“输出分布”，无法挽回子模块内部的效率损失。</li>
</ul>
<p>相关实验数据显示，Pre-Norm架构下各层输入的标准差波动范围仅为Post-Norm的1/3，分布稳定性显著更优。</p>
<h3 id="4-3-残差特征的累积模式：影响特征表达能力">4.3 残差特征的累积模式：影响特征表达能力</h3>
<p>两种范式对“残差融合后特征”的处理方式不同，导致特征的“累积逻辑”存在本质差异：</p>
<ul>
<li>
<p><strong>Post-Norm：增量累加模式</strong><br>
归一化强制各层输出尺度统一，特征以“增量式”累积——深层特征是在浅层特征的基础上“精细化提纯”（如强化关键语义、弱化噪声），每层都能为特征表达增加“有效增量”，从而提升模式的深层表达能力。</p>
</li>
<li>
<p><strong>Pre-Norm：总量累积模式</strong><br>
归一化仅作用于子模块输入，残差输出的尺度会随层数递增（因特征持续累积且无强制压缩）——当层数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span>足够大时，新增层对特征表达的贡献会被“总量稀释”，进而出现“深度虚化”现象：即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>层模型的实际表达能力与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span>层模型相近，无法体现“深度带来的优势”。这也是部分超深层Pre-Norm模型需结合“深度自适应缩放”等技术的原因。</p>
</li>
</ul>
<h2 id="五-层归一化位置选择的典型设计与效果">五、层归一化位置选择的典型设计与效果</h2>
<h3 id="5-1-基础二元范式-pre-norm与post-norm">5.1 基础二元范式 Pre-Norm与Post-Norm</h3>
<table>
<thead>
<tr>
<th>设计维度</th>
<th>Post-Norm（原始设计）</th>
<th>Pre-Norm（改进设计）</th>
</tr>
</thead>
<tbody>
<tr>
<td>核心顺序</td>
<td>子模块→残差→归一化</td>
<td>归一化→子模块→残差</td>
</tr>
<tr>
<td>梯度稳定性</td>
<td>深层易消失，支持≤30层</td>
<td>梯度无衰减，支持≥100层</td>
</tr>
<tr>
<td>收敛速度</td>
<td>慢（需200k步收敛）</td>
<td>快（120k步即可收敛）</td>
</tr>
<tr>
<td>最终精度</td>
<td>高（GLUE基准平均89.2）</td>
<td>中（GLUE基准平均87.4）</td>
</tr>
<tr>
<td>特征一致性</td>
<td>强（中间层可直接使用）</td>
<td>弱（需额外归一化处理）</td>
</tr>
<tr>
<td>代表模型</td>
<td>BERT、原始Transformer</td>
<td>GPT-3、Llama 2</td>
</tr>
</tbody>
</table>
<p>Post-Norm的高精度源于其强正则化效果，归一化对残差输出的强制缩放相当于引入隐式正则项，提升了模型泛化能力</p>
<p>Pre-Norm的稳定性则成为大模型的必然选择。</p>
<h3 id="5-2-混合范式：模块级位置差异化设计">5.2 混合范式：模块级位置差异化设计</h3>
<p>为了兼顾稳定性和精度，研究者提出在同一Transformer块内采用差异化的位置选择，典型代表为HybridNorm:</p>
<p>(1)<strong>注意力子模块</strong>：采用采用QKV归一化(类似于Pre-Norm)，对Query、Key、Value分别采用归一化再计算注意力分数，稳定信息交互过程。</p>
<p>(2)<strong>FFN子模块</strong>：采用Post-Norm，在残差融合后再进行归一化，保留特征提纯能力。</p>
<p>这种设计的核心逻辑是，注意力机制对于输入分布更加敏感，需要Pre-Norm提升稳定性，而FFN需要承担特征转化功能，需要Post-Norm提升表达精度。</p>
<h3 id="5-3-动态范式：训练自适应位置调整">5.3 动态范式：训练自适应位置调整</h3>
<p>动态范式根据训练阶段和层深度自动适应切换位置，以Dynamic-LN为代表：</p>
<p>(1)训练初期(前10%轮次)：全层Pre-Norm,快速稳定梯度分布。<br>
(2)训练中期(10%~80%轮次)：浅层Pre-Norm（维持梯度稳定）+深层Post-Norm（提升语义精度）<br>
（3）训练后期（80%~100%轮次）：全层Post-Norm, 优化模型表达能力，最大化精度优势。</p>
<h2 id="六-现有的问题">六、现有的问题</h2>
<h3 id="6-1-pre-norm的表达能力瓶颈">6.1 Pre-Norm的表达能力瓶颈</h3>
<p><strong>深度虚化</strong>是Pre-Norm的固有缺陷：当层数t增大时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi>F</mi><mo stretchy="false">(</mo><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{t+1} = x_t+F(Norm(x_t))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>的累积效应导致新增层的特征贡献被稀释，使模型实际深度没有随着层数增加而有效提升。</p>
<p>实验显示：120 层 Pre-Norm 模型的有效深度仅相当于 60 层 Post-Norm 模型，制约了性能上限。</p>
<h3 id="6-2-post-norm的训练扩展性不足">6.2 Post-Norm的训练扩展性不足</h3>
<p>Post-Norm的梯度累积衰减问题难以根治：即使采用最优warmup策略和初始化方法，其支持的最大层数仍局限于50层以内。在千亿参数模型训练过程中，Post-Norm架构模型的低层参数更新幅度不足0.001，预训练知识无法有效地传递到低层，导致顶层过拟合、底层欠拟合的现象。</p>
<h3 id="6-3-复杂设计的工程适配问题">6.3 复杂设计的工程适配问题</h3>
<p>混合与动态范式虽然能提升性能，但也引入了新的工程问题：<br>
（1）动态位置切换需额外维护训练阶段状态，增加了分布式训练的同步成本。<br>
（2）模块级差异化设计导致算子异构性上升，GPU内存访问次数增加20%~30%。<br>
（3）多模态场景下，不同模态的特征分布差异使单一位置选择无法适配，需要根据模态类型动态调整归一化位置。</p>
<h2 id="七-解决方案">七、解决方案</h2>
<p>**自适应混合归一化（Adaptive Hybrid Normalization,AHN）**框架，通过阶段感知动态切换，模块定制归一化，模态自适应校准，三重机制实现综合优化。</p>
<h3 id="7-1-核心设计方案">7.1 核心设计方案</h3>
<p>AHN框架在Transformer块内构建多层次归一化位置决策系统，具体设计如下：</p>
<h4 id="阶段感知动态切换">阶段感知动态切换</h4>
<p>（1）基于训练损失变化率动态划分阶段：</p>
<ul>
<li>损失下降率&gt;1%/轮：训练初期</li>
<li>0.1~1%/轮：训练中期</li>
<li>&lt;0.1%/轮：训练后期<br>
（2）依据阶段采用不同的归一化位置：</li>
<li>初期：全层采用Pre-Norm+RMSNorm组合（RMSNorm剔除均值计算，提升效率）</li>
<li>中期：浅层Pre-Norm，中层HybridNorm，深层Post-Norm</li>
<li>后期：全层Post-Norm，启用梯度裁剪（阈值1.0）维持稳定。</li>
</ul>
<h4 id="模块定制归一化">模块定制归一化</h4>
<p>（1）注意力模块：采用QKV归一化+可学习缩放因子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">γ_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">γ_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">γ_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，对注意力分数进行缩放，提升模型表达能力。<br>
(2)FFN模块：采用Post-Norm+动态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi></mrow><annotation encoding="application/x-tex">ε</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ε</span></span></span></span>，当输入方差＜0.5时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi></mrow><annotation encoding="application/x-tex">ε</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ε</span></span></span></span>从1e-5增加至1e-4，避免数值波动。<br>
（3）残差连接：引入自适应权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，随层数递增从0.8增至1.2，补偿pre-Norm的特征稀释效益。</p>
<h4 id="模态自适应校准">模态自适应校准</h4>
<p>（1）为不同模态设计专属预处理：图像特征归一化至0-1，音频特征*5缩放至-5~5。<br>
（2）模态专属<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">γ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>：为不同模态设计专属缩放因子，提升模型在各自模态上的表达能力。<br>
（3）跨模态共享校准层，计算全局均值/方差，对不同模态归一化结果二次对齐，减少分布差异</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kirihara-yun.github.io">Kirihara</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kirihara-yun.github.io/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/">https://kirihara-yun.github.io/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://kirihara-yun.github.io" target="_blank">Kirihara</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/">层归一化</a><a class="post-meta__tags" href="/tags/%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/">位置选择</a><a class="post-meta__tags" href="/tags/Transformer%E6%9E%B6%E6%9E%84/">Transformer架构</a></div><div class="post-share"><div class="social-share" data-image="/img/huiyuanai.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/09/25/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/436%E5%AF%BB%E6%89%BE%E5%8F%B3%E5%8C%BA%E9%97%B4/" title="436寻找右区间"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">436寻找右区间</div></div><div class="info-2"><div class="info-item-1">436寻找右区间 题目描述 给你一个数组intervals，其中intervals[i] = [starti, endi]，表示第i个区间的开始和结束。 区间i的右区间是满足startj &gt;= endi的最小的j,注意i可能等于j。 返回一个由每个区间i的右区间下表组成的数组，如果i不存在这样的右区间，则返回-1。 题目分析 （1）核心需求：寻找每个区间的右区间； （2）这是典型的下界查找问题，可以使用排序+二分查找来解决这个问题。 解题思路 （1）收集所有区间的起始点和索引 （2）对所有区间的起始点进行排序 （3）遍历每一个区间，使用二分查找找到第一个大于等于endi的起始点的索引 （4）如果找到，则返回对应的索引，否则返回-1 trick （1）使用sort来进行排序 （2）使用lower_bound来进行二分查找 代码实现 1234567891011121314151617181920class Solution &#123;public:    vector&lt;int&gt; findRightInterval(vector&lt;vector&lt;int&g...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/huiyuanai.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Kirihara</div><div class="author-info-description">喜欢您来</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Kirihara-Yun"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Kirihara-Yun" target="_blank" title=""><i class="fab fa-github"></i></a><a class="social-icon" href="/qinzhy26@mail2.sysu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">希望您天天开心</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">层归一化位置选择：Transformer架构中的关键设计决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9%E7%9A%84%E6%A0%B8%E5%BF%83%E5%AE%9A%E4%B9%89"><span class="toc-text">一、层归一化位置选择的核心定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BD%9C%E7%94%A8"><span class="toc-text">二、层归一化位置选择的核心作用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%B0%83%E6%8E%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-text">2.1 调控模型训练稳定性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E6%94%B6%E6%95%9B%E6%95%88%E7%8E%87"><span class="toc-text">2.2 优化模型收敛效率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%A0%A1%E5%87%86%E4%B8%AD%E9%97%B4%E5%B1%82%E7%89%B9%E5%BE%81%E5%88%86%E5%B8%83"><span class="toc-text">2.3 校准中间层特征分布</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-%E4%B8%BA%E4%BD%95%E5%BF%85%E9%A1%BB%E9%87%8D%E8%A7%86%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-text">三、为何必须重视层归一化位置选择？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8Elayernorm%E7%9A%84%E6%A0%B8%E5%BF%83%E9%80%BB%E8%BE%91%E5%86%B2%E7%AA%81"><span class="toc-text">3.1 残差连接与LayerNorm的核心逻辑冲突</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E4%B8%8E%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A1%8C%E6%80%A7%E7%9A%84%E9%80%82%E9%85%8D%E9%9C%80%E6%B1%82"><span class="toc-text">3.2 模型规模与训练可行性的适配需求</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9%E7%9A%84%E4%BD%9C%E7%94%A8%E6%9C%BA%E7%90%86"><span class="toc-text">四、层归一化位置选择的作用机理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%A2%AF%E5%BA%A6%E8%B7%AF%E5%BE%84%E5%88%86%E7%A6%BB%E6%9C%BA%E5%88%B6%EF%BC%9Apre-norm%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8A%BF"><span class="toc-text">4.1 梯度路径分离机制：Pre-Norm的核心优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB%E7%9A%84%E6%8A%91%E5%88%B6%E6%97%B6%E6%9C%BA%EF%BC%9A%E5%86%B3%E5%AE%9A%E6%94%B6%E6%95%9B%E6%95%88%E7%8E%87%E7%9A%84%E5%85%B3%E9%94%AE"><span class="toc-text">4.2 分布偏移的抑制时机：决定收敛效率的关键</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%AE%8B%E5%B7%AE%E7%89%B9%E5%BE%81%E7%9A%84%E7%B4%AF%E7%A7%AF%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%BD%B1%E5%93%8D%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B"><span class="toc-text">4.3 残差特征的累积模式：影响特征表达能力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9%E7%9A%84%E5%85%B8%E5%9E%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%88%E6%9E%9C"><span class="toc-text">五、层归一化位置选择的典型设计与效果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%9F%BA%E7%A1%80%E4%BA%8C%E5%85%83%E8%8C%83%E5%BC%8F-pre-norm%E4%B8%8Epost-norm"><span class="toc-text">5.1 基础二元范式 Pre-Norm与Post-Norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%B7%B7%E5%90%88%E8%8C%83%E5%BC%8F%EF%BC%9A%E6%A8%A1%E5%9D%97%E7%BA%A7%E4%BD%8D%E7%BD%AE%E5%B7%AE%E5%BC%82%E5%8C%96%E8%AE%BE%E8%AE%A1"><span class="toc-text">5.2 混合范式：模块级位置差异化设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%8A%A8%E6%80%81%E8%8C%83%E5%BC%8F%EF%BC%9A%E8%AE%AD%E7%BB%83%E8%87%AA%E9%80%82%E5%BA%94%E4%BD%8D%E7%BD%AE%E8%B0%83%E6%95%B4"><span class="toc-text">5.3 动态范式：训练自适应位置调整</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-%E7%8E%B0%E6%9C%89%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">六、现有的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-pre-norm%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%E7%93%B6%E9%A2%88"><span class="toc-text">6.1 Pre-Norm的表达能力瓶颈</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-post-norm%E7%9A%84%E8%AE%AD%E7%BB%83%E6%89%A9%E5%B1%95%E6%80%A7%E4%B8%8D%E8%B6%B3"><span class="toc-text">6.2 Post-Norm的训练扩展性不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%A4%8D%E6%9D%82%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%B7%A5%E7%A8%8B%E9%80%82%E9%85%8D%E9%97%AE%E9%A2%98"><span class="toc-text">6.3 复杂设计的工程适配问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-text">七、解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E6%A0%B8%E5%BF%83%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88"><span class="toc-text">7.1 核心设计方案</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%B6%E6%AE%B5%E6%84%9F%E7%9F%A5%E5%8A%A8%E6%80%81%E5%88%87%E6%8D%A2"><span class="toc-text">阶段感知动态切换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%AE%9A%E5%88%B6%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">模块定制归一化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E6%80%81%E8%87%AA%E9%80%82%E5%BA%94%E6%A0%A1%E5%87%86"><span class="toc-text">模态自适应校准</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/06/transformer/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9/" title="层归一化位置选择">层归一化位置选择</a><time datetime="2025-10-06T02:00:00.000Z" title="发表于 2025-10-06 10:00:00">2025-10-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/25/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/372%E8%B6%85%E7%BA%A7%E6%AC%A1%E6%96%B9/" title="372超级次方">372超级次方</a><time datetime="2025-09-24T16:00:00.000Z" title="发表于 2025-09-25 00:00:00">2025-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/25/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/436%E5%AF%BB%E6%89%BE%E5%8F%B3%E5%8C%BA%E9%97%B4/" title="436寻找右区间">436寻找右区间</a><time datetime="2025-09-24T16:00:00.000Z" title="发表于 2025-09-25 00:00:00">2025-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/20/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/memag/" title="MEMAGmemory-Efficient Graph Transformation via Access Pattern-Aware Optimization for DNNs">MEMAGmemory-Efficient Graph Transformation via Access Pattern-Aware Optimization for DNNs</a><time datetime="2025-09-20T08:00:00.000Z" title="发表于 2025-09-20 16:00:00">2025-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/18/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/318%E6%9C%80%E5%A4%A7%E5%8D%95%E8%AF%8D%E9%95%BF%E5%BA%A6%E4%B9%98%E7%A7%AF/" title="318. 最大单词长度乘积">318. 最大单词长度乘积</a><time datetime="2025-09-18T02:00:00.000Z" title="发表于 2025-09-18 10:00:00">2025-09-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/fushishan.jpg);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Kirihara</span></div><div class="footer_custom_text">你终于找到我啦(*^▽^*)</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>